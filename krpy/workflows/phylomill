#!/usr/bin/env python
# -*- coding: utf-8 -*-


from __future__ import print_function

LOG_LINE_SEP = '################################################################################'

################################################################################


if __name__ == '__main__':

    import platform
    import sys

    MAC = False
    if (platform.system() == 'Darwin'):
        MAC = True

    # P_V_MAJOR = sys.version_info.major
    # P_V_MINOR = sys.version_info.minor
    # P_V_MICRO = sys.version_info.micro

    # P_DNLD_MESSGE = 'Incompatible Python version: ' + sys.version.split('\n')[0] + '\n' + 'Download latest Python 2.X.X version: https://www.python.org/downloads'

    # if P_V_MAJOR != 2:
    #     print(P_DNLD_MESSGE)
    #     sys.exit(0)

    # if P_V_MINOR < 7:
    #     print(P_DNLD_MESSGE)
    #     sys.exit(0)

    # if MAC and (P_V_MICRO < 6):
    #     print(P_DNLD_MESSGE)
    #     sys.exit(0)

    import os
    import inspect
    import imp
    import argparse
    import subprocess

    from subprocess import call

    ############################################################################

    PS = os.path.sep

    # Script filename
    SCRIPT_FILE_PATH = inspect.getfile(inspect.currentframe())

    # Script directory path
    SCRIPT_DIR_PATH = os.path.dirname(os.path.abspath(SCRIPT_FILE_PATH))

    # krpy directory path
    KRPY_DIR_PATH = PS.join(SCRIPT_DIR_PATH.split(PS)[0:-1])

    # krpy root directory path
    KRPY_ROOT_DIR_PATH = PS.join(SCRIPT_DIR_PATH.split(PS)[0:-2])

    # print(SCRIPT_FILE_PATH)
    # print(SCRIPT_DIR_PATH)
    # print(KRPY_DIR_PATH)
    # print(KRPY_ROOT_DIR_PATH)

    ############################################################################

    PARSER = argparse.ArgumentParser()

    PARSER.add_argument(
        '-p',
        '--project_dir',
        type=str,
        help="Prepares clean project directory if it doesn't exist. Otherwise \
              sets the project directory.")

    PARSER.add_argument(
        '-c',
        '--commands',
        type=str,
        help='Commands to run.')

    PARSER.add_argument(
        '--gi',
        type=int,
        help='NCBI record GI.')

    PARSER.add_argument(
        '--flat',
        action='store_true',
        help='')

    PARSER.add_argument(
        '--raw',
        action='store_true',
        help='')

    PARSER.add_argument(
        '--locus',
        type=str,
        help='')

    PARSER.add_argument(
        '--gene_tree',
        action='store_true',
        help='')

    PARSER.add_argument(
        '--species_tree',
        action='store_true',
        help='')

    PARSER.add_argument(
        '--standalone',
        action='store_true',
        help='')

    ARGS = PARSER.parse_args()

    ############################################################################

    STANDALONE = False
    if ARGS.standalone:
        STANDALONE = ARGS.standalone

    ############################################################################

    COMMANDS = None
    if ARGS.commands:
        COMMANDS = set([x.strip() for x in ARGS.commands.split(',')])

    ############################################################################

    # import krpy path
    sys.path.insert(0, KRPY_ROOT_DIR_PATH)

    ############################################################################

    # Standalone?

    ####################
    # os.name
    # 'posix'

    # platform.system()
    # 'Darwin'

    # platform.release()
    # '13.3.0'
    ####################

    ####################
    # Bio
    # bs4
    # datrie
    # Levenshtein
    # unidecode
    # scipy
    # numpy
    ####################

    MAC_STANDALONE_DIR_PATH = KRPY_DIR_PATH + PS + 'standalone-deps-darwin-13.3.0' + PS

    found_Bio = False
    try:
        imp.find_module('Bio')
        found_Bio = True
    except ImportError:
        found_Bio = False

    found_bs4 = False
    try:
        imp.find_module('bs4')
        found_bs4 = True
    except ImportError:
        found_bs4 = False

    found_datrie = False
    try:
        imp.find_module('datrie')
        found_datrie = True
    except ImportError:
        found_datrie = False

    found_Levenshtein = False
    try:
        imp.find_module('Levenshtein')
        found_Levenshtein = True
    except ImportError:
        found_Levenshtein = False

    found_unidecode = False
    try:
        imp.find_module('unidecode')
        found_unidecode = True
    except ImportError:
        found_unidecode = False

    found_scipy = False
    try:
        imp.find_module('scipy')
        found_scipy = True
    except ImportError:
        found_scipy = False

    found_numpy = False
    try:
        imp.find_module('numpy')
        found_numpy = True
    except ImportError:
        found_numpy = False

    if MAC:

        if not (found_Bio and found_bs4 and found_datrie and found_Levenshtein and found_unidecode and found_scipy and found_numpy):

            sys.path.insert(0, MAC_STANDALONE_DIR_PATH + 'python')
            STANDALONE = True

    if STANDALONE:

        if MAC:
            print('Running PhyloMill in standalone mode.')

            # Unarchive standalone dependencies
            if not os.path.exists(MAC_STANDALONE_DIR_PATH):
                cmd = 'tar xzf ' + KRPY_DIR_PATH + PS + 'standalone-deps-darwin-13.3.0.tar.gz -C ' + KRPY_DIR_PATH
                call(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    shell=True)

        else:
            print('Standalone mode not yet supported for this operating system.')

    ############################################################################

    import shutil
    import copy
    import ConfigParser
    import shlex

    import multiprocessing
    from multiprocessing import Process
    from multiprocessing import JoinableQueue

    from krpy import KRSequenceDatabase
    from krpy.workflow_functions import wf_phylo as wf
    from krpy import krncbi
    from krpy import krother
    from krpy import krbioio
    from krpy import krbionames
    from krpy import krio
    from krpy import krcl
    from krpy import kralign
    from krpy import krblast

    from krpy.krother import write_log

    ############################################################################

    GI = None
    if ARGS.gi:
        GI = ARGS.gi

    LOCUS = None
    if ARGS.locus:
        LOCUS = ARGS.locus

    FLAT = None
    if ARGS.flat:
        FLAT = ARGS.flat

    RAW = None
    if ARGS.raw:
        RAW = ARGS.raw

    GENE_TREE = None
    if ARGS.gene_tree:
        GENE_TREE = ARGS.gene_tree

    SPECIES_TREE = None
    if ARGS.species_tree:
        SPECIES_TREE = ARGS.species_tree

    ############################################################################

    DB = None
    PRJ_DIR_PATH = None

    # Prepare clean project directory
    if ARGS.project_dir:
        PRJ_DIR_PATH = os.path.abspath(ARGS.project_dir).rstrip(PS)
        PRJ_DIR_PATH = PRJ_DIR_PATH + PS
    else:
        print('Project directory is required.')
        # write_log('Project directory is required.', LFP, newlines_before=0, newlines_after=0)
        sys.exit(0)

    # Log file path
    LOGS_DIR = PRJ_DIR_PATH + 'logs' + PS
    LFP = LOGS_DIR + 'main_log.txt'

    if ARGS.project_dir:

        if os.path.exists(PRJ_DIR_PATH):

            if (not os.path.exists(PRJ_DIR_PATH + 'db.sqlite3')) or (not os.path.exists(PRJ_DIR_PATH + 'config')):
                print('This does not appear to be a proper PhyloMill project directory: ' + PRJ_DIR_PATH)
                sys.exit(0)

            msg = 'Using project directory at ' + PRJ_DIR_PATH.rstrip(PS)
            write_log(msg=msg, log_file_path=LFP, append=True,
                newlines_before=0, newlines_after=0)
            DB = KRSequenceDatabase.KRSequenceDatabase(
                PRJ_DIR_PATH + 'db.sqlite3')

        else:

            prj_template_dir_path = SCRIPT_DIR_PATH.strip('workflows') + \
                'data' + PS + 'PhyloMill_template'

            shutil.copytree(prj_template_dir_path, PRJ_DIR_PATH, symlinks=False,
                            ignore=None)

            print('')
            msg = 'Creating project directory at ' + PRJ_DIR_PATH.rstrip(PS)
            write_log(msg=msg, log_file_path=LFP, append=False,
                newlines_before=0, newlines_after=0)

            DB = KRSequenceDatabase.KRSequenceDatabase(PRJ_DIR_PATH + 'db.sqlite3')

            wd = os.getcwd()
            os.chdir(PRJ_DIR_PATH+'organism_name_files')
            call(['./get_ncbi_data.sh'], stdout=open(os.devnull, 'wb'))
            os.chdir(wd)

            # sys.exit(0)

    ############################################################################

    if not COMMANDS:
        print('No commands given.')
        sys.exit(0)

    ############################################################################

    # Autopilot
    if 'autopilot' in COMMANDS:
        COMMANDS = set(['search', 'resolve_org_names', 'extract_loci', 'flatten', 'align', 'concatenate', 'raxml'])
        SPECIES_TREE = True

    ############################################################################

    DNLD_DIR_PATH = PRJ_DIR_PATH + 'downloaded_files' + PS
    OUT_DIR_PATH = PRJ_DIR_PATH + 'output' + PS
    SRCH_DIR_PATH = PRJ_DIR_PATH + 'search_strategies' + PS
    ORGN_DIR_PATH = PRJ_DIR_PATH + 'organism_name_files' + PS
    TEMP_DIR_PATH = PRJ_DIR_PATH + 'temporary_files' + PS

    ORG_LOC_DIR_PATH = OUT_DIR_PATH + '01_locus_files' + PS
    ALN_DIR_PATH = OUT_DIR_PATH + '02_alignments' + PS
    TRE_DIR_PATH = OUT_DIR_PATH + '03_trees' + PS
    PART_TRE_DIR_PATH = TRE_DIR_PATH + 'concat_partitioned' + PS
    # AST_TRE_DIR_PATH = TRE_DIR_PATH + 'astral' + PS

    CFG_FILE_PATH = PRJ_DIR_PATH + 'config'
    BLACKLIST_GIS_FILE_PATH = PRJ_DIR_PATH + 'blacklisted_gis.csv'
    CAT_ALN_FILE_PREFIX = ALN_DIR_PATH + 'concat'

    # RAxML_CMD_FILE_PATH = ALN_DIR_PATH + 'raxml-commands.txt'

    ############################################################################

    # Get configuration information
    CFG = ConfigParser.SafeConfigParser(allow_no_value=True)
    CFG.optionxform=str
    CFG.read(CFG_FILE_PATH)

    EMAIL = CFG.get('General', 'email')
    MAX_SEQ_LENGTH = CFG.getint('General', 'max_seq_length')

    # ASTRAL_JAR_FILE = CFG.get('General', 'astral_jar_file')

    # Hacks
    hacks_items = CFG.items('Hacks')
    hacks_temp = list()
    for hack in hacks_items:
        h = hack[0].split('_')
        if h[-1] != 'data':
            if CFG.getboolean('Hacks', '_'.join(h)):
                hacks_temp.append('_'.join(h))

    HACKS = dict()

    for hack in hacks_temp:
        if CFG.has_option('Hacks', hack + '_data'):
            HACKS[hack] = CFG.get('Hacks', hack + '_data')
        else:
            HACKS[hack] = None

    # Outgroup Taxa
    msg = 'Parsing outgroup taxa.'
    write_log(msg, LFP, newlines_before=0, newlines_after=0)

    out_temp = CFG.items('Outgroup Taxa')
    out_temp = [x[0] for x in out_temp]
    OUT_TAX_IDS = list()
    for out in out_temp:
        if out.isdigit():
            OUT_TAX_IDS.append(out)
        else:

            out_orig = out

            out_id = list()
            i = 0
            while (not out_id) and i < 3:
                i = i + 1
                out_id = list(krncbi.esearch(out, 'taxonomy', EMAIL))
                if not out_id:
                    out = out.replace('subsp. ', '')
                    out = out.replace('var. ', '')
            if out_id:
                out_id = out_id[0]

                OUT_TAX_IDS.append(str(out_id))
                msg = 'NCBI taxonomy ID for ' + out + ' is ' + str(out_id)
                write_log(msg, LFP, newlines_before=0, newlines_after=0)

                krio.replace_line_in_file(
                    file_path=CFG_FILE_PATH,
                    line_str=out_orig,
                    replace_str='# ' + out + '\n' + str(out_id))

    # Main Taxa
    msg = 'Parsing main taxa.'
    write_log(msg, LFP, newlines_before=0, newlines_after=0)

    tax_temp = CFG.items('Main Taxa')
    tax_temp = [x[0] for x in tax_temp]
    TAX_IDS = list()
    for tax in tax_temp:
        if tax.isdigit():
            TAX_IDS.append(tax)
        else:

            tax_orig = tax

            tax_id = list()
            i = 0
            while (not tax_id) and i < 3:
                i = i + 1
                tax_id = list(krncbi.esearch(tax, 'taxonomy', EMAIL))
                if not tax_id:
                    tax = tax.replace('subsp. ', '')
                    tax = tax.replace('var. ', '')
            if tax_id:
                tax_id = tax_id[0]

                TAX_IDS.append(str(tax_id))
                msg = 'NCBI taxonomy ID for ' + tax + ' is ' + str(tax_id)
                write_log(msg, LFP, newlines_before=0, newlines_after=0)

                krio.replace_line_in_file(
                    file_path=CFG_FILE_PATH,
                    line_str=tax_orig,
                    replace_str='# ' + tax + '\n' + str(tax_id))

    # Excluded Taxa
    msg = 'Parsing excluded taxa'
    write_log(msg, LFP, newlines_before=0, newlines_after=0)

    exclude_temp = CFG.items('Excluded Taxa')
    exclude_temp = [x[0] for x in exclude_temp]
    EXCLUDE_TAX_IDS = list()
    for exclude_tax in exclude_temp:
        if exclude_tax.isdigit():
            EXCLUDE_TAX_IDS.append(exclude_tax)
        else:

            exclude_tax_orig = exclude_tax

            exclude_id = list()
            i = 0
            while (not exclude_id) and i < 3:
                i = i + 1
                exclude_id = list(krncbi.esearch(exclude_tax, 'taxonomy', EMAIL))
                if not exclude_id:
                    exclude_tax = exclude_tax.replace('subsp. ', '')
                    exclude_tax = exclude_tax.replace('var. ', '')
            if exclude_id:
                exclude_id = exclude_id[0]

                EXCLUDE_TAX_IDS.append(str(exclude_id))
                msg = 'NCBI taxonomy ID for ' + exclude_tax + ' is ' + str(exclude_id)
                write_log(msg, LFP, newlines_before=0, newlines_after=0)

                krio.replace_line_in_file(
                    file_path=CFG_FILE_PATH,
                    line_str=exclude_tax_orig,
                    replace_str='# ' + exclude_tax + '\n' + str(exclude_id))

    ALL_TAX_IDS = OUT_TAX_IDS + TAX_IDS

    # Organism name resolution
    SYN = list()
    syn_temp = None
    if CFG.has_option('Organism Names', 'synonymize'):
        syn_temp = CFG.get('Organism Names', 'synonymize')
    if syn_temp:
        SYN = syn_temp.split(',')
        SYN = [x.strip().lower() for x in SYN]
    SKIP_PREV_SYN = CFG.getboolean('Organism Names',
        'skip_previously_synonymized')
    REM_HYBRIDS = CFG.getboolean('Organism Names', 'remove_hybrids')
    REM_NONSPECIFIC = CFG.getboolean('Organism Names', 'remove_nonspecific')
    MERGE_SUBSPECIES = CFG.getboolean('Organism Names', 'merge_subspecies')
    MERGE_VARIETIES = CFG.getboolean('Organism Names', 'merge_varieties')

    # Loci
    LOCI = dict()
    loci_temp = CFG.items('Loci')
    loci_temp = [(x[0], float(x[1])) for x in loci_temp]

    AUTO_LOCI = False
    if not loci_temp:
        # Automatically find loci
        AUTO_LOCI = True
        loci_temp = [(x['full'], 0.80) for x in krio.parse_directory(SRCH_DIR_PATH, '$$$')]

    for l in loci_temp:
        l_similarity = l[1]
        l = l[0]
        locus_ss_file_path = SRCH_DIR_PATH + l
        if os.path.exists(locus_ss_file_path):
            locus_cfg = ConfigParser.SafeConfigParser(allow_no_value=True)
            locus_cfg.optionxform=str
            locus_cfg.read(locus_ss_file_path)

            l_dict = dict()
            l_strategies = list()

            l_dict['name'] = l
            l_dict['similarity'] = l_similarity

            for l_sec in locus_cfg.sections():
                if l_sec == l:
                    l_db = locus_cfg.get(l, 'database')
                    l_query = locus_cfg.get(l, 'query')
                    # l_short_name = locus_cfg.get(l, 'short_name')
                    l_dict['database'] = l_db
                    l_dict['query'] = l_query
                    # l_dict['short_name'] = l_short_name
                    # l_bad_gis = locus_cfg.get(l_sec, 'bad_gis')
                    # l_bad_gis = l_bad_gis.split(',')
                    # l_bad_gis = [int(x.strip()) for x in l_bad_gis]
                    # l_dict['bad_gis'] = l_bad_gis
                    # l_dict['min_length'] = 0
                    # if locus_cfg.has_option(section=l_sec, option='min_length'):
                    #     l_dict['min_length'] = locus_cfg.getint(l_sec, 'min_length')
                    # l_dict['max_length'] = 0
                    # if locus_cfg.has_option(section=l_sec, option='max_length'):
                    #     l_dict['max_length'] = locus_cfg.getint(l_sec, 'max_length')
                # elif l_sec == 'blast':
                #     l_blast_dbs = locus_cfg.get(l_sec, 'databases')
                #     l_blast_dbs = l_blast_dbs.split(',')
                #     l_blast_dbs = [x.strip() for x in l_blast_dbs]
                #     l_dict['blast_dbs'] = l_blast_dbs
                else:
                    l_lrp = locus_cfg.getint(l_sec, 'locus_relative_position')
                    l_ft = locus_cfg.get(l_sec, 'feature_type')
                    l_ql = locus_cfg.get(l_sec, 'qualifier_label')
                    l_qv = locus_cfg.get(l_sec, 'qualifier_value')
                    l_re = locus_cfg.getboolean(l_sec, 'regex')
                    l_svm = locus_cfg.getboolean(l_sec, 'strict_value_match')
                    l_ml = locus_cfg.getint(l_sec, 'min_length')
                    l_el = locus_cfg.getint(l_sec, 'extra_length')

                    l_strat_dict = {
                    'locus_relative_position': l_lrp,
                    'feature_type': l_ft,
                    'qualifier_label': l_ql,
                    'qualifier_value': l_qv,
                    'regex': l_re,
                    'strict_value_match': l_svm,
                    'min_length': l_ml,
                    'extra_length': l_el
                    }

                    l_strategies.append(l_strat_dict)

            l_dict['strategies'] = l_strategies

            LOCI[l] = l_dict
        else:
            write_log('There is no search strategy file for locus: ' + l, LFP)
            sys.exit(0)

    if AUTO_LOCI:

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'No loci given, determining automatically.'
        write_log(msg, LFP)

        MIN_SEQS_AUTO_LOCI = CFG.getint('General', 'min_seqs_for_auto_locus_discovery')

        for locus_name in LOCI.keys():

            ncbi_db = LOCI[locus_name]['database']
            query_term_str = LOCI[locus_name]['query']

            min_seq_length = None
            strategies = LOCI[locus_name]['strategies']
            for s in strategies:
                l_msl = s['min_length']
                if not min_seq_length:
                    min_seq_length = l_msl
                else:
                    min_seq_length = min(l_msl, min_seq_length)

            # msg = 'Searching NCBI ' + ncbi_db + ' database for ' + \
            #       locus_name + '.'
            # write_log(msg, LFP, newlines_before=0, newlines_after=0)

            gis = wf.search_genbank(
                ncbi_db=ncbi_db,
                query_term_str=query_term_str,
                ncbi_tax_ids=TAX_IDS,
                exclude_tax_ids=EXCLUDE_TAX_IDS,
                min_seq_length=min_seq_length,
                max_seq_length=MAX_SEQ_LENGTH,
                email=EMAIL,
                log_file_path=LFP)

            found_count = len(gis)

            msg = 'Found ' + str(found_count) + ' records for locus ' + locus_name + '.'
            write_log(msg, LFP)

            if found_count < MIN_SEQS_AUTO_LOCI:
                del LOCI[locus_name]

        cfg_loci_names = list()

        for locus_name in LOCI.keys():
            cfg_loci_names.append(locus_name)

        cfg_loci_names.sort(key=lambda x: x.lower())

        cfg_loci_string = '[Loci]\n\n' + ' = 0.80\n'.join(cfg_loci_names) + ' = 0.80'
        print_loci_string = ', '.join(cfg_loci_names)

        msg = 'Will use loci: ' + print_loci_string + '.'
        write_log(msg, LFP)

        krio.replace_line_in_file(
            file_path=CFG_FILE_PATH,
            line_str='[Loci]',
            replace_str=cfg_loci_string)


    # Efficiently utilize CPU cores
    number_of_loci = len(LOCI.keys())

    CPU_TOTAL = CFG.get('General', 'cpu')

    if CPU_TOTAL == 'auto':
        CPU_TOTAL = multiprocessing.cpu_count()
    else:
        CPU_TOTAL = int(CPU_TOTAL)

    CPU_FOR_PYTHON = 1
    CPU_FOR_OTHER = 1

    if (CPU_TOTAL == 2) or (CPU_TOTAL == 3):
        CPU_FOR_PYTHON = 1
        CPU_FOR_OTHER = 2
    # elif CPU_TOTAL < 8:
    #     CPU_FOR_PYTHON = 2
    #     CPU_FOR_OTHER = CPU_TOTAL / 2
    elif CPU_TOTAL > 3:
        o = 0
        c = 0
        cpu_optimized = False
        for o in range(2, 2048):
            for c in range(1, number_of_loci+1):
                # print(o, c, c * o)
                if c * o >= CPU_TOTAL:
                    cpu_optimized = True
                    if c * o == CPU_TOTAL:
                        c = c+1
                        o = o+1
                    break
            if cpu_optimized:
                break
        CPU_FOR_PYTHON = max(1, c-1)
        CPU_FOR_OTHER = max(2, o-1)

    # print('CPU_TOTAL', CPU_TOTAL)
    # print('CPU_FOR_PYTHON', CPU_FOR_PYTHON)
    # print('CPU_FOR_OTHER', CPU_FOR_OTHER)
    # print('number_of_loci', number_of_loci)

    # MAFFT executable
    MAFFT_ALN_PROG_EXE = CFG.get('General', 'mafft_executable')
    if STANDALONE:
        if MAC:
            MAFFT_ALN_PROG_EXE = MAC_STANDALONE_DIR_PATH + 'other' + PS + 'mafft-mac' + PS + 'mafft.bat'

    # Flatten options
    FLAT_ALN_PROG = CFG.get('Flatten', 'align_program')
    FLAT_ALN_PROG_EXE = CFG.get('General', FLAT_ALN_PROG + '_executable')

    if STANDALONE:
        if MAC:
            if FLAT_ALN_PROG == 'mafft':
                FLAT_ALN_PROG_EXE = MAC_STANDALONE_DIR_PATH + 'other' + PS + 'mafft-mac' + PS + 'mafft.bat'
            elif FLAT_ALN_PROG == 'muscle':
                FLAT_ALN_PROG_EXE = MAC_STANDALONE_DIR_PATH + 'other' + PS + 'muscle'
            elif FLAT_ALN_PROG == 'clustalo':
                FLAT_ALN_PROG_EXE = MAC_STANDALONE_DIR_PATH + 'other' + PS + 'clustalo'

    FLAT_ALN_PROG_OPTIONS = CFG.get('Flatten', 'align_program_options')
    if FLAT_ALN_PROG == 'mafft':
        options_temp = shlex.split(FLAT_ALN_PROG_OPTIONS)
        if '--thread' in options_temp:
            thread_index = options_temp.index('--thread')
            options_temp.pop(thread_index+1)
            options_temp.pop(thread_index)
        options_temp.append('--thread')
        options_temp.append(str(CPU_FOR_OTHER))
        FLAT_ALN_PROG_OPTIONS = ' '.join(options_temp)

    FLAT_RESOLVE_AMBIGUITIES = CFG.getboolean('Flatten', 'resolve_ambiguities')
    FLAT_ID_BOTTOM = 0.80
    FLAT_ID_TOP = 0.99

    # Align options
    ALN_ALN_PROG = CFG.get('Align', 'align_program')
    ALN_ALN_PROG_EXE = CFG.get('General', ALN_ALN_PROG + '_executable')

    if STANDALONE:
        if MAC:
            if ALN_ALN_PROG == 'mafft':
                ALN_ALN_PROG_EXE = MAC_STANDALONE_DIR_PATH + 'other' + PS + 'mafft-mac' + PS + 'mafft.bat'
            elif ALN_ALN_PROG == 'muscle':
                ALN_ALN_PROG_EXE = MAC_STANDALONE_DIR_PATH + 'other' + PS + 'muscle'
            elif ALN_ALN_PROG == 'clustalo':
                ALN_ALN_PROG_EXE = MAC_STANDALONE_DIR_PATH + 'other' + PS + 'clustalo'

    ALN_ALN_PROG_OPTIONS = CFG.get('Align', 'align_program_options')
    if ALN_ALN_PROG == 'mafft':
        options_temp = shlex.split(ALN_ALN_PROG_OPTIONS)
        if '--thread' in options_temp:
            thread_index = options_temp.index('--thread')
            options_temp.pop(thread_index+1)
            options_temp.pop(thread_index)
        options_temp.append('--thread')
        options_temp.append(str(CPU_TOTAL))
        ALN_ALN_PROG_OPTIONS = ' '.join(options_temp)

    ALN_TAX_NAME_LIST = CFG.get('Align', 'taxon_name')
    ALN_TAX_NAME_LIST = ALN_TAX_NAME_LIST.split(',')
    ALN_TAX_NAME_LIST = [x.strip() for x in ALN_TAX_NAME_LIST]

    # RAxML
    RAXML_EXE = CFG.get('General', 'raxml_executable')

    if STANDALONE:
        if MAC:
            RAXML_EXE = MAC_STANDALONE_DIR_PATH + 'other' + PS + 'raxml'

    # Blast

    BLAST_LOCI = dict()
    blast_temp = CFG.items('Blast')
    for b in blast_temp:
        if b[1] != '':
            BLAST_LOCI[b[0]] = float(b[1])

    ############################################################################

    # Reset database
    if 'reset' in COMMANDS:
        msg = 'Resetting database.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)
        DB.close()
        os.remove(PRJ_DIR_PATH + 'db.sqlite3')
        DB = KRSequenceDatabase.KRSequenceDatabase(PRJ_DIR_PATH + 'db.sqlite3')

    ############################################################################

    # Autopilot
    if 'autopilot' in COMMANDS:
        COMMANDS = set(['search', 'resolve_org_names', 'extract_loci', 'flatten', 'align', 'concatenate', 'raxml'])
        SPECIES_TREE = True

    ############################################################################

    # Blacklist gis in blacklisted_gis.csv file

    gis = krio.read_table_file(
        path=BLACKLIST_GIS_FILE_PATH,
        has_headers=False,
        headers=None,
        delimiter=',',
        quotechar='"',
        rettype='set')

    # with open(BLACKLIST_GIS_FILE_PATH, 'r') as f:
    #     lines = f.readlines()
    #     for l in lines:
    #         if (not l.startswith('#')) and (not l.strip() == ''):
    #             gis.append(l.strip('\n'))

    gis = list(gis)

    wf.blacklist_gis(gis=gis, kr_seq_db_object=DB, log_file_path=LFP)

    ############################################################################

    # Search genbank
    if 'search' in COMMANDS:

        for locus_name in LOCI.keys():

            # ln_split = locus_name.split('_')
            # if len(ln_split) > 1 and ln_split[1] == 'blast':

            #     wf.blast_search(
            #         kr_seq_db_object=DB,
            #         loci=LOCI,
            #         locus_name=locus_name,
            #         ncbi_tax_ids=ALL_TAX_IDS,
            #         max_seq_length=MAX_SEQ_LENGTH,
            #         email=EMAIL,
            #         log_file_path=LFP,
            #         dnld_dir_path=DNLD_DIR_PATH,
            #         temp_dir=TEMP_DIR_PATH)

            # else:

            wf.regular_search(
                kr_seq_db_object=DB,
                log_file_path=LFP,
                email=EMAIL,
                loci=LOCI,
                locus_name=locus_name,
                ncbi_tax_ids=ALL_TAX_IDS,
                exclude_tax_ids=EXCLUDE_TAX_IDS,
                max_seq_length=MAX_SEQ_LENGTH,
                dnld_dir_path=DNLD_DIR_PATH)

        COMMANDS.add('resolve_org_names')

    ############################################################################

    # Get more sequences using BLAST
    if 'blast' in COMMANDS:

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'Will run blastn to enrich loci database.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        for locus_name in BLAST_LOCI.keys():

            msg = 'Loading records for locus ' + locus_name + ', this may take a bit.'
            write_log(msg, LFP, newlines_before=0, newlines_after=0)

            locus_dir_path = ORG_LOC_DIR_PATH + locus_name + PS
            krio.prepare_directory(locus_dir_path)

            e_value = BLAST_LOCI[locus_name]
            locus_dict = LOCI[locus_name]

            # Get all unflattened records for current locus
            records = DB.get_records_with_annotations(
                annotation_type='locus',
                annotation=locus_name,
                active=True,
                inactive=False)

            ref_recs_file_path = ORG_LOC_DIR_PATH + locus_name + '__reference.fasta'

            reference_records = wf.produce_reference_sequences(
                aln_executable=MAFFT_ALN_PROG_EXE,
                locus_name=locus_name,
                records=records,
                ref_recs_file_path=ref_recs_file_path,
                log_file_path=LFP)

            blast_databses = ['nt', 'est']

            for db in blast_databses:

                msg = 'Running blastn on ' + db + ' database with E-value of ' + str(e_value) + '.'
                write_log(msg, LFP, newlines_before=0, newlines_after=0)

                blast_results_path = TEMP_DIR_PATH + locus_name + '__' + db + '.xml'

                query_str = wf.produce_ncbi_query_string(
                    ncbi_tax_ids=ALL_TAX_IDS,
                    exclude_tax_ids=EXCLUDE_TAX_IDS,
                    query_term_str=locus_dict['query'],
                    min_seq_length=100,  ## QUICK HACK!!!
                    max_seq_length=MAX_SEQ_LENGTH)

                cmd = 'blastn -remote -task blastn -entrez_query \'' + query_str + '\'' \
                    ' -db ' + db + \
                    ' -query ' + ref_recs_file_path + \
                    ' -out ' + blast_results_path + \
                    ' -evalue ' + str(e_value) + \
                    ' -outfmt 5'

                call(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    shell=True)

                blast_gis = krblast.gis_from_blast_results(
                    file_name=blast_results_path,
                    min_sequence_length=-1, max_sequence_length=-1)

                timestamp = krother.timestamp()
                timestamp = timestamp.replace('-', '_')
                timestamp = timestamp.replace(':', '_')
                timestamp = timestamp.replace(' ', '_')

                dnld_file_name = locus_name + '_blast_' + db + '_' + timestamp + '.gb'
                dnld_file_path = DNLD_DIR_PATH + dnld_file_name

                records_to_add = wf.download_new_records(
                    locus_name=locus_name,
                    ncbi_db=locus_dict['database'],
                    gis=blast_gis,
                    dnld_file_path=dnld_file_path,
                    kr_seq_db_object=DB,
                    email=EMAIL,
                    log_file_path=LFP)

                if len(records_to_add) > 0:

                    msg = 'Annotating blast hits. This may take a bit.'
                    write_log(msg, LFP)

                    records_to_add_dict = dict()
                    for r in records_to_add:
                        records_to_add_dict[r.annotations['gi']] = r

                    records_to_add_dict = krblast.annotate_blast_hits(
                        blast_results_xml=blast_results_path,
                        gb_records=records_to_add_dict,
                        annotation_type='blast_hit')

                    records_to_add_dict = krblast.merge_blast_hit_annotations(
                        gb_records=records_to_add_dict,
                        annotation_type_to_merge='blast_hit',
                        annotation_type_merged='pwfx',
                        merged_label='blast_hits_merged',
                        qualifiers_dict={'note':locus_name + '|x'})

                    msg = 'Adding downloaded records to database. This may take a bit.'
                    write_log(msg, LFP)

                    record_count = len(records_to_add)
                    for i, record in enumerate(records_to_add_dict.values()):

                        krcl.print_progress(
                            current=i+1, total=record_count, length=0,
                            prefix=krother.timestamp() + ' ',
                            postfix=' - ' + record.annotations['gi'],
                            show_bar=False)

                        DB.add_genbank_record(
                            record=record,
                            action_str='Blast ' + db + ' search result.')

                        gi = int(record.annotations['gi'])

                        DB.add_record_annotation(
                            record_reference=gi,
                            type_str='locus',
                            annotation_str=locus_name,
                            record_reference_type='gi')

                        DB.add_record_annotation(
                            record_reference=gi,
                            type_str='source_file',
                            annotation_str=dnld_file_name,
                            record_reference_type='gi')

                    print()

                DB.save()

        COMMANDS.add('resolve_org_names')

    ############################################################################

    # Resolve organism names
    if 'resolve_org_names' in COMMANDS:

        ########################################################################

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'Resolving organism names.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        # DB._DB_CURSOR.execute('BEGIN TRANSACTION;')

        ########################################################################

        org_ids = DB.db_get_row_ids(table_name='organisms',
            where_dict={'active': 1})

        if not org_ids:
            msg = 'There are no organisms in the database. Was search conducted?'
            write_log(msg, LFP, newlines_before=0, newlines_after=0)
            sys.exit(1)


        if len(org_ids) < 2:
            msg = 'There are only ' + str(len(org_ids)) + ' organisms in the database. Was search conducted?'
            write_log(msg, LFP, newlines_before=0, newlines_after=0)
            sys.exit(1)

        ########################################################################

        wf.update_lineage_info(
            kr_seq_db_object=DB,
            log_file_path=LFP,
            email=EMAIL)

        ########################################################################

        ncbi_names_table = None
        synonymy_table = None

        if len(SYN) > 0:

            msg = 'Loading synonymy table and NCBI organism name list.'
            write_log(msg, LFP, newlines_before=0, newlines_after=0)

            ncbi_names_table = krio.read_table_file(
                path=ORGN_DIR_PATH + 'ncbi_tax_names',
                has_headers=False,
                headers=('tax_id', 'name_txt', 'unique_name', 'name_class'),
                delimiter='\t|',
                quotechar=None,
                stripchar='"',
                rettype='dict')

            synonymy_table = krio.read_table_file(
                path=ORGN_DIR_PATH + 'synonymy.csv',
                has_headers=True, headers=None, delimiter=',')

        ########################################################################

        taxid_blacklist_set = krio.read_table_file(
            path=ORGN_DIR_PATH + 'taxid_blacklist.tsv',
            has_headers=False,
            headers=None,
            delimiter=',',
            quotechar='"',
            rettype='set')

        ########################################################################

        record_taxon_mappings_list = krio.read_table_file(
            path=ORGN_DIR_PATH + 'record_taxon_mappings.tsv',
            has_headers=False,
            headers=('id', 'taxon'),
            delimiter='\t',
            quotechar=None,
            stripchar='"',
            rettype='dict')
        record_taxon_mappings_dict = dict()
        for t in record_taxon_mappings_list:
            record_taxon_mappings_dict[t['id']]=t['taxon']

        ########################################################################

        taxid_taxon_mappings_list = krio.read_table_file(
            path=ORGN_DIR_PATH + 'taxid_taxon_mappings.tsv',
            has_headers=False,
            headers=('taxid', 'taxon'),
            delimiter='\t',
            quotechar=None,
            stripchar='"',
            rettype='dict')
        taxid_taxon_mappings_dict = dict()
        for t in taxid_taxon_mappings_list:
            taxid_taxon_mappings_dict[t['taxid']]=t['taxon']

        ########################################################################

        taxonomy_cache = dict()

        ########################################################################

        if 'tgrc' in HACKS.keys():

            write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

            msg = 'Using "Tomato Genetics Resource Center" to resolve organism names.'
            write_log(msg, LFP, newlines_before=0, newlines_after=0)

            wf.rename_tgrc_organisms(
                kr_seq_db_object=DB,
                taxonomy_cache=taxonomy_cache,
                log_file_path=LFP,
                email=EMAIL
                )

        ########################################################################

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'Checking for record -> taxon mappings.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        wf.rename_organisms_with_record_taxon_mappings(
            kr_seq_db_object=DB,
            record_taxon_mappings_dict=record_taxon_mappings_dict,
            taxonomy_cache=taxonomy_cache,
            log_file_path=LFP,
            email=EMAIL
            )

        ########################################################################

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'Checking for tax_id -> taxon mappings and synonymy.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        wf.rename_organisms_using_taxids(
            kr_seq_db_object=DB,
            taxid_blacklist_set=taxid_blacklist_set,
            taxid_taxon_mappings_dict=taxid_taxon_mappings_dict,
            taxonomy_cache=taxonomy_cache,
            log_file_path=LFP,
            email=EMAIL,
            rem_hybrids=REM_HYBRIDS,
            rem_nonspecific=REM_NONSPECIFIC,
            merge_subspecies=MERGE_SUBSPECIES,
            merge_varieties=MERGE_VARIETIES,
            skip_prev_syn=SKIP_PREV_SYN,
            tax_groups_to_syn=SYN,
            authority_alternates_file = ORGN_DIR_PATH + \
                'authority_alternates.dat',
            ncbi_names_table=ncbi_names_table,
            synonymy_table=synonymy_table)

        ########################################################################

        wf.update_lineage_info(
            kr_seq_db_object=DB,
            log_file_path=LFP,
            email=EMAIL)

        ########################################################################

        DB.save()

        krcl.clear_line()
        msg = 'Organism name check: done.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        ########################################################################

    ############################################################################

    # Extract loci
    if 'extract_loci' in COMMANDS:

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'Extracting loci.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        krio.prepare_directory(ORG_LOC_DIR_PATH)

        ###

        processes = list()
        queue = JoinableQueue()

        ###

        def t(q):
            while True:
                locus_name = q.get()

                lfp = LOGS_DIR + locus_name + '_log.txt'

                db_thread = KRSequenceDatabase.KRSequenceDatabase(PRJ_DIR_PATH + 'db.sqlite3')

                ln_split = locus_name.split('_')
                if len(ln_split) > 1 and ln_split[1] == 'blast':
                    continue

                # write_log(LOG_LINE_SEP, lfp, newlines_before=0, newlines_after=0)

                msg = 'Loading records for locus ' + locus_name + ', this may take a bit.'
                write_log(msg, lfp, newlines_before=0, newlines_after=0)

                locus_dict = LOCI[locus_name]

                records = db_thread.get_records_with_annotations(
                    annotation_type='locus_not_extracted',
                    annotation=locus_name,
                    active=True,
                    inactive=False)

                seed_recs_file_path = ORG_LOC_DIR_PATH + locus_name + '__seeds.fasta'

                acc_rej_gi_dict = wf.extract_loci(
                    aln_executable=MAFFT_ALN_PROG_EXE,
                    locus_dict=locus_dict,
                    records=records,
                    log_file_path=lfp,
                    kr_seq_db_object=db_thread,
                    temp_dir=TEMP_DIR_PATH,
                    seed_recs_file_path=seed_recs_file_path,
                    cpu=CPU_FOR_OTHER)

                rej_gi_list = acc_rej_gi_dict['reject']
                delete_note = 'failed sequence similarity test'
                rej_gi_list = [[int(x[1]), delete_note] for x in rej_gi_list]

                no_feature_gi_list = acc_rej_gi_dict['no_feature']
                delete_note = 'no locus annotation'
                no_feature_gi_list = [[x, delete_note] for x in no_feature_gi_list]

                short_feature_gi_list = acc_rej_gi_dict['short_feature']
                delete_note = 'locus annotation is too short'
                short_feature_gi_list = [[x, delete_note] for x in short_feature_gi_list]

                bad_list = rej_gi_list + no_feature_gi_list + short_feature_gi_list

                # print()

                for bad in bad_list:

                    bad_gi = bad[0]
                    delete_note = bad[1]

                    msg = 'inactivating:' + \
                    ' gi:' + str(bad_gi) + ' note:' + delete_note
                    write_log(msg, lfp)

                    where_dict = {'ncbi_gi': bad_gi}

                    blacklist_notes = delete_note

                    rec_ids = db_thread.db_get_row_ids(
                        'records',
                        where_dict=where_dict)

                    db_thread.set_inactive(
                        table_name='records',
                        where_dict=where_dict)

                    if rec_ids:

                        for rec_id in rec_ids:

                            rec = db_thread.get_record(
                                record_reference=rec_id,
                                record_reference_type='raw'
                                )

                            db_thread.add_record_to_blacklist(
                                ncbi_gi=int(rec.annotations['gi']),
                                ncbi_version=rec.id,
                                internal_reference=rec.annotations['internal_reference'],
                                notes=blacklist_notes)

                db_thread.save()
                q.task_done()

        ###

        for locus_name in LOCI.keys():
            queue.put(locus_name)

        for i in range(CPU_FOR_PYTHON):
            worker = Process(target=t, args=(queue,))
            worker.start()
            processes.append(worker)

        queue.join()

        for p in processes:
            p.terminate()

        ###

        print(chr(27) + "[2K", end='\r')

    ############################################################################

    # Export blacklisted gis
    if 'export_blacklisted_gis' in COMMANDS:

        msg = 'Exporting blacklisted gis.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        blacklisted_gis = DB.db_select(
            table_name_list=['blacklist'],
            column_list=['ncbi_gi'],
            where_dict=None,
            join_rules_str=None,
            order_by_column_list=['ncbi_gi'])

        with open(BLACKLIST_GIS_FILE_PATH, 'wb') as f:
            for bgi in blacklisted_gis:
                f.write(str(bgi['ncbi_gi']) + '\n')

    ############################################################################

    # Export active records
    if 'export_active_records' in COMMANDS:

        msg = 'Exporting active records.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        timestamp = krother.timestamp()
        timestamp = timestamp.replace('-', '_')
        timestamp = timestamp.replace(':', '_')
        timestamp = timestamp.replace(' ', '_')

        gb_file_name = ''

        records = None
        if (not LOCUS) and (FLAT or RAW):
            records = DB.get_all_records(active=True, inactive=False)
            gb_file_name = 'active_records' + '_' + timestamp + '.gb'

        elif LOCUS and FLAT:
            records = DB.get_records_with_annotations(
                annotation_type='locus_flat',
                annotation=LOCUS,
                active=True,
                inactive=False)

            for rec in records:
                rec.description = rec.annotations['organism']

            gb_file_name = 'active_records' + '_' + 'locus_flat' + '_' + LOCUS + '_' + timestamp + '.gb'

        elif LOCUS and RAW:
            records = DB.get_records_with_annotations(
                annotation_type='locus',
                annotation=LOCUS,
                active=True,
                inactive=False)
            gb_file_name = 'active_records' + '_' + 'locus_raw' + '_' + LOCUS + '_' + timestamp + '.gb'

        ar_file_path = OUT_DIR_PATH + gb_file_name

        krbioio.write_sequence_file(
            records=records,
            file_path=ar_file_path,
            file_format='genbank')

    ############################################################################

    # Blacklist gi
    if 'blacklist_gi' in COMMANDS:

        if not GI:
            write_log('No GI given.', LFP, newlines_before=0,
                newlines_after=0)
            sys.exit(0)

        wf.blacklist_gis(gis=[GI], kr_seq_db_object=DB)

    ############################################################################

    # Whitelist gi
    if 'whitelist_gi' in COMMANDS:

        if not GI:
            write_log('No GI given.', LFP, newlines_before=0,
                newlines_after=0)
            sys.exit(0)

        whitelist_notes = 'user_activated'

        msg = 'activating:' + \
        ' gi:' + str(GI) + ' note:' + whitelist_notes
        write_log(msg, LFP)

        where_dict = {'ncbi_gi': GI}

        DB.set_active(
            table_name='records',
            where_dict=where_dict)

        DB.db_delete(
            table_name='blacklist',
            where_dict=where_dict)

        DB.save()

    ############################################################################

    # Produce one locus per organism
    if 'flatten' in COMMANDS:

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'Producing one locus per organism.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        ###

        processes = list()
        queue = JoinableQueue()

        ###

        def t(q):
            while True:
                locus_name = q.get()

                lfp = LOGS_DIR + locus_name + '_log.txt'

                db_thread = KRSequenceDatabase.KRSequenceDatabase(PRJ_DIR_PATH + 'db.sqlite3')

                # write_log(LOG_LINE_SEP, lfp, newlines_before=0, newlines_after=0)

                msg = 'Loading records for locus ' + locus_name + ', this may take a bit.'
                write_log(msg, lfp, newlines_before=0, newlines_after=0)

                locus_dir_path = ORG_LOC_DIR_PATH + locus_name + PS
                krio.prepare_directory(locus_dir_path)

                locus_dict = LOCI[locus_name]

                # Get all unflattened records for current locus
                records = db_thread.get_records_with_annotations(
                    annotation_type='locus',
                    annotation=locus_name,
                    active=True,
                    inactive=False)

                records = sorted(records, key=lambda x: len(x.seq), reverse=True)

                ###

                ref_recs_file_path = ORG_LOC_DIR_PATH + locus_name + '__reference.fasta'

                reference_records = wf.produce_reference_sequences(
                    aln_executable=MAFFT_ALN_PROG_EXE,
                    locus_name=locus_name,
                    records=records,
                    ref_recs_file_path=ref_recs_file_path,
                    log_file_path=lfp,
                    cpu=CPU_FOR_OTHER)

                ###

                org_locus_dict = dict()
                for record in records:
                    org_id = record.annotations['kr_seq_db_org_id']
                    if org_id not in org_locus_dict.keys():
                        org_locus_dict[org_id] = list()
                    org_locus_dict[org_id].append(record)

                # Get all flattened records for current locus
                records_flat = db_thread.get_records_with_annotations(
                    annotation_type='locus_flat',
                    annotation=locus_name,
                    active=True,
                    inactive=False)

                org_locus_flat_dict = dict()
                for record_flat in records_flat:
                    org_id = record_flat.annotations['kr_seq_db_org_id']
                    if org_id not in org_locus_flat_dict.keys():
                        org_locus_flat_dict[org_id] = list()
                    org_locus_flat_dict[org_id].append(record_flat)

                ####################################################################

                org_count = len(org_locus_dict.keys())
                for i, org_id in enumerate(org_locus_dict.keys()):

                    where_dict = {'organisms.id': int(org_id)}
                    org_dict = db_thread.get_organisms(where_dict=where_dict)[0]
                    org_flat = krbionames.flatten_organism_name(
                        parsed_name=org_dict, sep=' ')

                    org_records = org_locus_dict[org_id]

                    krcl.print_progress(
                        current=i+1, total=org_count, length=0,
                        prefix=krother.timestamp() + ' ',
                        postfix=' - ' + org_flat + ' - ' + str(len(org_records)) + ' records.',
                        show_bar=False)
                    # print()

                    aln_name = locus_name + '__' + org_flat.replace(' ', '_')

                    aln_file_path = locus_dir_path + aln_name + '.phy'
                    seq_file_path = locus_dir_path + aln_name + '.fasta'

                    ################################################################

                    deleted_rec_ids = False
                    new_rec_ids = False

                    seq_match = True
                    aln_match = True

                    seq_file_exists = False
                    aln_file_exists = False

                    ################################################################

                    # Check if there already is a flat record for this organism
                    if org_id in org_locus_flat_dict:
                        # print('flat_record_exists', org_flat)
                        flat_rec_list = org_locus_flat_dict[org_id]
                        flat_rec = flat_rec_list[0]
                        flat_rec_id = int(flat_rec.annotations['kr_seq_db_id'])

                        db_aln = None

                        redo_aln = False

                        if len(flat_rec_list) > 1:
                            print('More than one flat record!')
                            pass
                        else:

                            parent_rec_ids = db_thread.get_parent_rec_ids(rec_id=flat_rec_id)

                            rec_ids_in_db = [int(x.annotations['kr_seq_db_id']) for x in org_records]
                            rec_ids_in_db = sorted(rec_ids_in_db, reverse=False)

                            # Check if there is a locus alignment/sequence file
                            seq_file_exists = os.path.exists(seq_file_path)
                            aln_file_exists = os.path.exists(aln_file_path)

                            if (not seq_file_exists) and (not aln_file_exists):
                                redo_aln = True

                            existing_record_list = list()

                            if seq_file_exists:
                                existing_record_list = krbioio.read_sequence_file(
                                    file_path=seq_file_path,
                                    file_format='fasta',
                                    ret_type='list',
                                    key='gi')

                            elif aln_file_exists:
                                existing_record_list = krbioio.read_alignment_file(
                                    file_path=aln_file_path,
                                    file_format='phylip-relaxed')

                            gis_in_file = list()
                            if existing_record_list:
                                gis_in_file = [int(x.id) for x in existing_record_list]

                            rec_id_gi_dict = dict()
                            rec_ids_in_file = list()
                            for gi_in_file in gis_in_file:
                                rec_id_in_file = db_thread.db_get_row_ids(
                                    table_name='records',
                                    where_dict={'ncbi_gi': gi_in_file})[0]
                                rec_ids_in_file.append(rec_id_in_file)
                                rec_id_gi_dict[str(rec_id_in_file)] = gi_in_file
                            rec_ids_in_file = sorted(rec_ids_in_file, reverse=False)

                            ########################################################

                            # Check if there are any deleted records
                            if seq_file_exists or aln_file_exists:
                                if set(rec_ids_in_file) == set(parent_rec_ids):
                                    # print('no records were deleted')
                                    pass
                                else:
                                    deleted_rec_ids = list(set(rec_ids_in_file) - set(parent_rec_ids))
                                    # if not (len(deleted_rec_ids) > 0):
                                    deleted_rec_ids = deleted_rec_ids + list(set(parent_rec_ids) - set(rec_ids_in_file))
                                    # print(len(deleted_rec_ids), 'deleted_rec_ids:', deleted_rec_ids)

                            # Check if there are any new records
                            if set(rec_ids_in_db) == set(parent_rec_ids):
                                # print('no new records')
                                pass
                            else:
                                new_rec_ids = list(set(rec_ids_in_db) - set(parent_rec_ids))
                                # print(len(new_rec_ids), 'new_rec_ids:', new_rec_ids)

                            # print(len(parent_rec_ids), 'parent_rec_ids:', parent_rec_ids)
                            # print(len(rec_ids_in_db), 'rec_ids_in_db:', rec_ids_in_db)
                            # print(len(rec_ids_in_file), 'rec_ids_in_file:', rec_ids_in_file)
                            # print('gis_in_file', gis_in_file)

                            ########################################################

                            # Check if there is an alignment associated with this
                            # record
                            aln_id = db_thread.db_get_row_ids(
                                table_name='alignments',
                                where_dict={'rec_id': flat_rec_id})

                            if aln_id:
                                aln_id = aln_id[0]
                                db_aln = db_thread.get_alignment(
                                    alignment_id=aln_id)

                            # print('aln_id', aln_id)

                            # If there is a locus alignment/sequence file,
                            # get all the sequences from this file and compare to
                            # the sequences in the database
                            if (seq_file_exists or aln_file_exists) and existing_record_list:
                                if db_aln:
                                    for a in db_aln:
                                        if str(a.id) in rec_id_gi_dict.keys():
                                            gi = str(rec_id_gi_dict[str(a.id)])
                                            for er in existing_record_list:
                                                if er.id == gi:
                                                    er_seq_str = str(er.seq).upper()
                                                    db_seq_str = str(a.seq).upper()
                                                    if er_seq_str != db_seq_str:
                                                        aln_match = False
                                    # print('aln_match', aln_match)
                                else:
                                    er = existing_record_list[0]
                                    er_seq_str = str(er.seq).upper()
                                    db_seq_str = str(flat_rec.seq).upper()
                                    if er_seq_str != db_seq_str:
                                        seq_match = False
                                    # print('seq_match', seq_match)

                        ############################################################

                        if deleted_rec_ids:
                            for del_rec_id in deleted_rec_ids:
                                del_rec = None

                                for r in org_records:
                                    if int(r.annotations['kr_seq_db_id']) == del_rec_id:
                                        del_rec = r
                                        break

                                if del_rec:
                                    org_records.remove(del_rec)

                                    blacklist_notes = 'user_deleted'

                                    ncbi_gi=int(del_rec.annotations['gi'])

                                    #### TODO: USED IN blacklist_gi SHOULD BE REFACTORED ####

                                    msg = 'inactivating:' + \
                                    ' gi:' + str(ncbi_gi) + ' note:' + blacklist_notes
                                    write_log(msg, lfp)

                                    where_dict = {'id': del_rec_id}

                                    db_thread.set_inactive(
                                        table_name='records',
                                        where_dict=where_dict)

                                    db_thread.add_record_to_blacklist(
                                        ncbi_gi=ncbi_gi,
                                        ncbi_version=del_rec.id,
                                        internal_reference=del_rec.annotations['internal_reference'],
                                        notes=blacklist_notes)

                                    where_dict = {'parent_rec_id': del_rec_id}

                                    db_thread.db_delete(
                                        table_name='record_ancestry',
                                        where_dict=where_dict)

                                    ####

                        rec_ids_in_db = [int(x.annotations['kr_seq_db_id']) for x in org_records]
                        rec_ids_in_db = sorted(rec_ids_in_db, reverse=False)
                        parent_rec_id_list = rec_ids_in_db

                        new_seq_str = None
                        flat_locus_produced = False
                        if (len(org_records) > 1) and (redo_aln or ((not aln_match) or new_rec_ids or deleted_rec_ids)):

                            if seq_file_exists:
                                os.remove(seq_file_path)
                            if aln_file_exists:
                                os.remove(aln_file_path)

                            new_aln = existing_record_list
                            if new_rec_ids or deleted_rec_ids or redo_aln:
                                records_to_align = list(existing_record_list)
                                for r in org_records:
                                    if redo_aln or (new_rec_ids and int(r.annotations['kr_seq_db_id']) in new_rec_ids):
                                        trimmed_rec = wf.trim_record_to_locus(
                                            record=r,
                                            locus_name=locus_name)
                                        records_to_align.append(trimmed_rec)

                                new_aln = wf.flatten_locus(
                                    org_name=org_flat,
                                    records=records_to_align,
                                    reference_records=reference_records,
                                    locus_dict=locus_dict,
                                    log_file_path=lfp,
                                    already_trimmed=True,
                                    aln_program=FLAT_ALN_PROG,
                                    aln_program_executable=FLAT_ALN_PROG_EXE,
                                    aln_options=FLAT_ALN_PROG_OPTIONS,
                                    min_locus_sequence_identity_range=[FLAT_ID_BOTTOM, FLAT_ID_TOP])

                                if new_aln[1] < FLAT_ID_BOTTOM:
                                    msg = 'Please review the ' + locus_name + ' alignment for ' + org_flat + '. Identity value: ' + str(new_aln[1])
                                    write_log(msg, lfp, newlines_before=0, newlines_after=0)

                                    if 'align' in COMMANDS:
                                        COMMANDS.remove('align')
                                    if 'concatenate' in COMMANDS:
                                        COMMANDS.remove('concatenate')

                                new_aln = new_aln[0]

                            # print(new_aln)
                            for seq_in_aln in new_aln:
                                # print(seq_in_aln.id)
                                seq_in_aln.id = seq_in_aln.id.strip('_R_')

                            wf.update_record_alignment(
                                rec_id=flat_rec_id,
                                new_aln=new_aln,
                                aln_name=aln_name,
                                kr_seq_db_object=db_thread)

                            consensus = kralign.consensus(
                                alignment=new_aln,
                                threshold=0.4,
                                unknown='',
                                resolve_ambiguities=FLAT_RESOLVE_AMBIGUITIES)

                            new_seq_str = str(consensus).upper()

                            flat_locus_produced = True

                            krbioio.write_alignment_file(
                                alignment=new_aln,
                                file_path=aln_file_path,
                                file_format='phylip-relaxed')

                        elif (len(org_records) == 1):

                            db_thread.db_delete(
                                table_name='alignments',
                                where_dict={'rec_id': flat_rec_id})

                            new_rec = None

                            if not seq_match:
                                er = existing_record_list[0]
                                new_seq_str = str(er.seq).upper()
                                new_rec = er

                            elif deleted_rec_ids or redo_aln:
                                trimmed_rec = wf.trim_record_to_locus(
                                    record=org_records[0],
                                    locus_name=locus_name)
                                if trimmed_rec:
                                    new_seq_str = str(trimmed_rec.seq)
                                    new_rec = trimmed_rec
                                    db_seq_str = str(flat_rec.seq).upper()
                                    if (new_seq_str.upper() != db_seq_str) or redo_aln:
                                        seq_match = False

                            if not seq_match:

                                flat_locus_produced = True

                                if seq_file_exists:
                                    os.remove(seq_file_path)
                                if aln_file_exists:
                                    os.remove(aln_file_path)

                                krbioio.write_sequence_file(
                                    records=[new_rec],
                                    file_path=seq_file_path,
                                    file_format='fasta')

                        if flat_locus_produced:

                            where_dict = {'rec_id': flat_rec_id}
                            db_thread.db_delete(
                                table_name='record_ancestry',
                                where_dict=where_dict)

                            for parent_rec_id in parent_rec_id_list:
                                values_dict = {
                                    'rec_id': flat_rec_id,
                                    'parent_rec_id': parent_rec_id
                                    }
                                db_thread.db_insert('record_ancestry', values_dict)

                            db_thread.db_delete(
                                table_name='sequences',
                                where_dict={'rec_id': flat_rec_id})

                            sequence_alphabet_str = db_thread.bio_alphabet_to_string(
                                bio_alphabet=flat_rec.seq.alphabet)

                            seq_id = db_thread.add_sequence(
                                rec_id=flat_rec_id,
                                sequence_str=new_seq_str,
                                sequence_alphabet_str=sequence_alphabet_str)[0]

                            repr_list = db_thread.produce_seq_edits(new_seq_str, new_seq_str)

                            seq_rep_id = db_thread.add_sequence_representation(
                                rec_id=flat_rec_id,
                                seq_id=seq_id,
                                repr_list=repr_list)[0]

                        if len(org_records) == 0:

                            if seq_file_exists:
                                os.remove(seq_file_path)
                            if aln_file_exists:
                                os.remove(aln_file_path)

                            db_thread.db_delete(
                                table_name='records',
                                where_dict={'id': flat_rec_id})

                        ############################################################

                    org_records_count = len(org_records)
                    if (org_records_count > 0) and (org_id not in org_locus_flat_dict):

                        # print('flat_record_does_not_exist')

                        flat_locus_produced = False

                        seq_rep_id_list = list()
                        # print(org_id, org_flat, org_records_count)

                        new_seq_str = ''
                        sequence_alphabet_str = db_thread.bio_alphabet_to_string(
                            bio_alphabet=org_records[0].seq.alphabet)

                        ############################################################

                        if org_records_count > 1:

                            aln = wf.flatten_locus(
                                org_name=org_flat,
                                records=org_records,
                                reference_records=reference_records,
                                locus_dict=locus_dict,
                                log_file_path=lfp,
                                aln_program=FLAT_ALN_PROG,
                                aln_program_executable=FLAT_ALN_PROG_EXE,
                                aln_options=FLAT_ALN_PROG_OPTIONS,
                                min_locus_sequence_identity_range=[FLAT_ID_BOTTOM, FLAT_ID_TOP])

                            if aln:

                                if aln[1] < FLAT_ID_BOTTOM:
                                    msg = 'Please review the ' + locus_name + ' alignment for ' + org_flat + '. Identity value: ' + str(aln[1])
                                    write_log(msg, lfp, newlines_before=0, newlines_after=0)

                                    if 'align' in COMMANDS:
                                        COMMANDS.remove('align')
                                    if 'concatenate' in COMMANDS:
                                        COMMANDS.remove('concatenate')

                                aln = aln[0]

                                flat_locus_produced = True

                                # print(aln)
                                for seq_in_aln in aln:
                                    # print(seq_in_aln.id)
                                    seq_in_aln.id = seq_in_aln.id.strip('_R_')

                                krbioio.write_alignment_file(
                                    alignment=aln,
                                    file_path=aln_file_path,
                                    file_format='phylip-relaxed')

                                for ar in aln:

                                    seq = db_thread.get_sequence_for_record(
                                        record_reference=int(ar.id),
                                        record_reference_type='gi'
                                        )

                                    seq_rep_list = db_thread.produce_seq_edits(
                                        s1=str(seq).upper(),
                                        s2=str(ar.seq).upper())

                                    rec_id = db_thread.db_get_row_ids(
                                        table_name='records',
                                        where_dict={'ncbi_gi': int(ar.id)})[0]

                                    seq_id = db_thread.db_get_row_ids(
                                        table_name='sequences',
                                        where_dict={'rec_id': rec_id})[0]

                                    seq_rep_id = db_thread.add_sequence_representation(
                                        seq_id=seq_id,
                                        repr_list=seq_rep_list,
                                        rec_id=None,
                                        aln_id=None)[0]

                                    seq_rep_id_list.append(seq_rep_id)

                                consensus = kralign.consensus(
                                    alignment=aln,
                                    threshold=0.4,
                                    unknown='',
                                    resolve_ambiguities=FLAT_RESOLVE_AMBIGUITIES)

                                new_seq_str = str(consensus)

                        else:

                            trimmed_rec = wf.trim_record_to_locus(
                                record=org_records[0],
                                locus_name=locus_name)

                            if trimmed_rec:

                                flat_locus_produced = True

                                new_seq_str = str(trimmed_rec.seq)
                                krbioio.write_sequence_file(
                                    records=[trimmed_rec],
                                    file_path=seq_file_path,
                                    file_format='fasta')

                        ############################################################

                        if flat_locus_produced:

                            internal_reference = locus_name + '_' + krother.random_id(6)
                            parent_rec_id_list = [int(x.annotations['kr_seq_db_id']) for x in org_records]

                            rec_id = db_thread.add_record(
                                org_id=org_id,
                                ncbi_gi=None,
                                ncbi_version=None,
                                internal_reference=internal_reference,
                                description=None,
                                sequence_str=new_seq_str,
                                sequence_alphabet_str=sequence_alphabet_str,
                                parent_rec_id_list=parent_rec_id_list,
                                active=True,
                                action_str='Record for locus')

                            db_thread.add_record_annotation(
                                record_reference=rec_id,
                                type_str='locus_flat',
                                annotation_str=locus_name,
                                record_reference_type='raw')

                            if len(seq_rep_id_list) > 1:
                                db_thread.add_alignment(
                                    name=aln_name,
                                    seq_rep_id_list=seq_rep_id_list,
                                    description=None,
                                    rec_id=rec_id)

                    db_thread.save()

                q.task_done()

            ####################################################################

        ###

        for locus_name in LOCI.keys():
            queue.put(locus_name)

        for i in range(CPU_FOR_PYTHON):
            worker = Process(target=t, args=(queue,))
            worker.start()
            processes.append(worker)

        queue.join()

        for p in processes:
            p.terminate()

        ###

        print(chr(27) + "[2K", end='\r')

    ############################################################################

    # ORG_LOC_DIR_PATH = OUT_DIR_PATH + '01_locus_files' + PS
    # ALN_DIR_PATH = OUT_DIR_PATH + '02_alignments' + PS
    # TRE_DIR_PATH = OUT_DIR_PATH + '03_trees' + PS
    # PART_TRE_DIR_PATH = TRE_DIR_PATH + 'partitioned' + PS
    # AST_TRE_DIR_PATH = TRE_DIR_PATH + 'astral' + PS

    # Align sequences
    if 'align' in COMMANDS:

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'Preparing inter-taxon locus alignments.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        krio.prepare_directory(ALN_DIR_PATH)
        krio.prepare_directory(TRE_DIR_PATH)

        for locus_name in LOCI.keys():

            aln_file_path_phy = ALN_DIR_PATH + locus_name + '.phy'
            aln_file_path_fas = ALN_DIR_PATH + locus_name + '.fasta'

            if os.path.exists(aln_file_path_phy) or os.path.exists(aln_file_path_fas):
                msg = 'Alignment file for locus ' + locus_name + ' already exists.'
                write_log(msg, LFP, newlines_before=0, newlines_after=0)
                continue

            msg = 'Aligning locus ' + locus_name + '.'
            write_log(msg, LFP, newlines_before=0, newlines_after=0)

            # Get all flattened records for current locus
            records_flat = DB.get_records_with_annotations(
                annotation_type='locus_flat',
                annotation=locus_name,
                active=True,
                inactive=False)

            if len(records_flat) < 2:
                msg = 'There are ' + str(len(records_flat)) + ' records for locus ' + locus_name + ". Was 'flatten' command run?"
                write_log(msg, LFP, newlines_before=0, newlines_after=0)
                sys.exit(1)

            for r in records_flat:

                binomial = r.annotations['organism'].replace(' ', '_')
                common = r.annotations['common_name'].replace(', ', '|').replace(' ', '_').replace('\'', '')

                lineage = r.annotations['lineage'].split('$$$')
                lineage_parsed = krncbi.parse_lineage_string_list(
                    lineage_string_list=lineage)[0]

                outgroup = ''
                # print(lineage_parsed)
                # print(binomial)
                lineage_taxids = [x['taxid'] for x in lineage_parsed]
                org_id = int(r.annotations['kr_seq_db_org_id'])
                org_dict = DB.get_organisms(where_dict={'organisms.id': org_id})[0]
                org_taxids = org_dict['ncbi_tax_ids']
                lineage_taxids = org_taxids + lineage_taxids

                for lineage_taxid in lineage_taxids:
                    if str(lineage_taxid) in OUT_TAX_IDS:
                        outgroup = '||OUT'

                # print(binomial, outgroup, ' :: ', common, ' :: ', lineage_parsed)

                name_terms = list()
                for term in ALN_TAX_NAME_LIST:
                    name_term = term
                    if term == 'scientific':
                        name_term = binomial
                    elif term == 'common':
                        name_term = common
                    else:
                        for lp in lineage_parsed:
                            if ('rank' in lp.keys()) and (lp['rank'] == term):
                                name_term = lp['name']
                                break
                    name_terms.append(name_term)

                # print(binomial, common, name_term)
                # print('--- --- --- --- --- --- ---')

                # r.id = binomial + '||' + common + '||' + name_term

                r.id = '||'.join(name_terms) + outgroup

                r.description = ''
                r.name = ''

            # This will write sequences to be aligned to fasta file
            # krbioio.write_sequence_file(
            #     records=records_flat,
            #     file_path=aln_file_path_fas,
            #     file_format='fasta')

            aln = kralign.align(
                records=records_flat,
                program=ALN_ALN_PROG,
                options=ALN_ALN_PROG_OPTIONS,
                program_executable=ALN_ALN_PROG_EXE)

            krbioio.write_alignment_file(
                alignment=aln,
                file_path=aln_file_path_phy,
                file_format='phylip-relaxed')

            krbioio.write_alignment_file(
                alignment=aln,
                file_path=aln_file_path_fas,
                file_format='fasta')

            outgroup_taxa = wf.get_outgroup_names(alignment=aln)

            ####################################################################

            # Write RAxML commands file

            msg = 'Writing RAxML input files for locus ' + locus_name + '.'
            write_log(msg, LFP, newlines_before=0, newlines_after=0)

            loc_tre_dir_path = TRE_DIR_PATH + locus_name + PS
            krio.prepare_directory(loc_tre_dir_path)

            wf.produce_raxml_input_files(
                exe=RAXML_EXE,
                name=locus_name,
                aln_file_path=aln_file_path_phy,
                out_dir_path=loc_tre_dir_path,
                outgroup_taxa=outgroup_taxa,
                partitions=None,
                threads=CPU_TOTAL)

            ####################################################################

    ############################################################################

    # ORG_LOC_DIR_PATH = OUT_DIR_PATH + '01_locus_files' + PS
    # ALN_DIR_PATH = OUT_DIR_PATH + '02_alignments' + PS
    # TRE_DIR_PATH = OUT_DIR_PATH + '03_trees' + PS
    # PART_TRE_DIR_PATH = TRE_DIR_PATH + 'partitioned' + PS
    # AST_TRE_DIR_PATH = TRE_DIR_PATH + 'astral' + PS

    # Concatenate alignments
    if 'concatenate' in COMMANDS:

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        msg = 'Concatenating alignments.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        aln_list = list()
        name_list = list()
        aln_and_name_list = list()

        for locus_name in LOCI.keys():

            msg = 'Loading alignment for locus ' + locus_name + '.'
            write_log(msg, LFP, newlines_before=0, newlines_after=0)

            aln_file_path = ALN_DIR_PATH + locus_name + '.phy'

            if not os.path.exists(aln_file_path):
                msg = 'No alignment file exists for locus ' + locus_name + '.'
                write_log(msg, LFP, newlines_before=0, newlines_after=0)
                sys.exit(1)

            aln = krbioio.read_alignment_file(
                file_path=aln_file_path,
                file_format='phylip-relaxed')

            aln_list.append(aln)
            name_list.append(locus_name)
            aln_and_name_list.append([aln, locus_name])

        ########################################################################

        # Produce presence/absence matrix
        presence_list = list()
        # length_list = list()
        for p in range(0, len(aln_and_name_list)):
            presence_list.append('0')
        matrix = dict()
        for a in aln_and_name_list:
            # length_list.append(str(a[0].get_alignment_length()))
            for s in a[0]:
                if not s.id in matrix:
                    matrix[s.id] = copy.copy(presence_list)
        for a in aln_and_name_list:
            for s in a[0]:
                idx = name_list.index(a[1])
                matrix[s.id][idx] = '1'
        matrix_output_file = ALN_DIR_PATH + 'concat_locus_presence' + '.csv'
        f = open(matrix_output_file, 'wb')
        f.write('taxon' + ',' + 'count' + ',' + ','.join(name_list) + '\n')
        # f.write('' + ',' + '' + ',' + ','.join(length_list) + '\n')
        for key in matrix.keys():
            f.write(key + ',' + str(matrix[key].count('1')) + ',' +
                    ','.join(matrix[key]) + '\n')
        f.close()

        ########################################################################

        # Concatenate alignments

        msg = 'Concatenating.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        concatenated = None
        aln = None
        partitions = None

        if len(aln_list) > 1:
            concatenated = kralign.concatenate(
                alignments=aln_list,
                padding_length=0,
                partitions=None)

            aln = concatenated[0]
            partitions = concatenated[1]

        elif aln_list:
            aln = aln_list[0]
            partitions = [(1, aln.get_alignment_length())]

        krbioio.write_alignment_file(
            alignment=aln,
            file_path=CAT_ALN_FILE_PREFIX + '.phy',
            file_format='phylip-relaxed')

        krbioio.write_alignment_file(
            alignment=aln,
            file_path=CAT_ALN_FILE_PREFIX + '.fasta',
            file_format='fasta')

        ########################################################################

        outgroup_taxa = wf.get_outgroup_names(alignment=aln)

        ########################################################################

        # Write partitions files
        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)
        msg = 'Writing locus partition files.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        partitions_output_file = ALN_DIR_PATH + 'concat_locus_partitions' + '.csv'
        # raxml_partitions_output_file = ALN_DIR_PATH + 'locus-partitions-raxml' + '.txt'
        f_part = open(partitions_output_file, 'wb')
        # f_part_raxml = open(raxml_partitions_output_file, 'wb')
        f_part.write('locus,start,end\n')
        for i, part in enumerate(partitions):
            # raxml_part_line = 'DNA, ' + name_list[i] + ' = ' + str(part[0]) + '-' + str(part[1]) + '\n'
            # f_part_raxml.write(raxml_part_line)
            part_line = name_list[i] + ',' + str(part[0]) + ',' + str(part[1]) + '\n'
            f_part.write(part_line)
        f_part.close()
        # f_part_raxml.close()

        ########################################################################

        # Write RAxML commands file
        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)
        msg = 'Writing RAxML input files.'
        write_log(msg, LFP, newlines_before=0, newlines_after=0)

        krio.prepare_directory(PART_TRE_DIR_PATH)

        wf.produce_raxml_input_files(
            exe=RAXML_EXE,
            name='concat',
            aln_file_path=CAT_ALN_FILE_PREFIX + '.phy',
            out_dir_path=PART_TRE_DIR_PATH,
            outgroup_taxa=outgroup_taxa,
            partitions=partitions,
            threads=CPU_TOTAL,
            locus_name_list=name_list)

        ########################################################################

    ############################################################################

    # Run RAxML
    if ('raxml' in COMMANDS) and (GENE_TREE or SPECIES_TREE):

        write_log(LOG_LINE_SEP, LFP, newlines_before=0, newlines_after=0)

        commands_file_paths = list()

        if SPECIES_TREE:
            commands_file_paths.append(PART_TRE_DIR_PATH + 'RAxML_commands.txt')

        if GENE_TREE:
            for locus_name in LOCI.keys():
                commands_file_paths.append(TRE_DIR_PATH + locus_name + PS + 'RAxML_commands.txt')

        for cmd_file_path in commands_file_paths:

            if SPECIES_TREE:
                msg = 'Running RAxML on concatenated and partiotioned alignment.'
                write_log(msg, LFP, newlines_before=0, newlines_after=0)

            if GENE_TREE:
                msg = 'Running RAxML on ' + cmd_file_path.split(PS)[-2] + ' alignment.'
                write_log(msg, LFP, newlines_before=0, newlines_after=0)

            if not os.path.exists(cmd_file_path):
                msg = 'RAxML commands file does not exist. Were sequences aligned, concatenated?'
                write_log(msg, LFP, newlines_before=0, newlines_after=0)
                sys.exit(1)

            cmd = ''
            with open(cmd_file_path, 'rb') as f:
                cmd = f.readlines()
                cmd = [x.replace('\n', '').replace('\\', '').strip() for x in cmd]
                cmd = ' '.join(cmd)

            subprocess.call(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                shell=True)

    ############################################################################

    # # Run ASTRAL
    # if 'astral' in COMMANDS:

    #     krio.prepare_directory(AST_TRE_DIR_PATH)

    #     trees = list()
    #     for locus_name in LOCI.keys():
    #         tree_file_path = TRE_DIR_PATH + locus_name + PS + 'RAxML_' + locus_name + PS + 'RAxML_bestTree.' + locus_name

    #         if not os.path.exists(tree_file_path):
    #             msg = 'RAxML tree file does not exist for locus ' + locus_name + '.'
    #             write_log(msg, LFP, newlines_before=0, newlines_after=0)
    #             sys.exit(1)

    #         tre = ''
    #         with open(tree_file_path, 'rb') as f:
    #             tre = f.readline()
    #             tre = tre.replace('||', '___')
    #             trees.append(tre)

    #     input_trees_file = AST_TRE_DIR_PATH + 'gene_trees.tre'
    #     with open(input_trees_file, 'w') as f:
    #         f.writelines(trees)

    #     output_tree_file = AST_TRE_DIR_PATH + 'species_tree.tre'
    #     cmd = 'java -jar ' + ASTRAL_JAR_FILE + ' -i ' + input_trees_file + ' -o ' + output_tree_file

    #     subprocess.call(
    #         cmd,
    #         stdout=subprocess.PIPE,
    #         stderr=subprocess.PIPE,
    #         shell=True)

    ############################################################################

    DB.close()

    msg = 'Done.'
    write_log(msg, LFP, newlines_before=0, newlines_after=0)

    ############################################################################
